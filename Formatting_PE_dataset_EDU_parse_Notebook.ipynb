{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8ac26ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "import copy\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from transformers import BertTokenizer\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk import sent_tokenize\n",
    "from difflib import SequenceMatcher\n",
    "# import stanza\n",
    "# stanza_nlp = stanza.Pipeline('en')\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fef620ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'ArgumentAnnotatedEssays-2.0 3/brat-project-final/'\n",
    "all_filenames = [path+i.split(\".\")[0] for i in os.listdir(path) \\\n",
    "                 if (i.endswith(\"ann\") or i.endswith(\"txt\")) and i.startswith(\"essay\")]\n",
    "all_filenames = list(set(all_filenames))\n",
    "len(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d40ee91",
   "metadata": {},
   "outputs": [],
   "source": [
    "conj_adv = [\"therefore\", \"however\", \"moreover\", \"furthermore\", \"accordingly\", \"similarly\", \"also\", \"hence\", \"anyway\",\n",
    "\"nevertheless\", \"besides\", \"thereafter\", \"nonetheless\", \"consequently\", \"thus\", \"likewise\", \"further\", \"meanwhile\",\n",
    "           \"due to\"]\n",
    "conj_adv_pattern = \"(\"+\"|\".join([r\"\\b\" + i + r\"\\b\" for i in conj_adv])+\")\"\n",
    "\n",
    "def spacy_text_process(t1):\n",
    "    doc = nlp(t1)\n",
    "    return \" \".join([token.text for token in doc])\n",
    "\n",
    "def split_conj_adv(txt):\n",
    "    splits = re.split(conj_adv_pattern, txt)\n",
    "    all_splits = []\n",
    "    for ix, i in enumerate(splits):\n",
    "        if i in set(conj_adv) and ix+1 < len(splits):\n",
    "            splits[ix+1] = i+splits[ix+1]\n",
    "        else:\n",
    "            all_splits.append(i)\n",
    "    return all_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf380999",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 402/402 [00:32<00:00, 12.20it/s]\n"
     ]
    }
   ],
   "source": [
    "para_dict = {}\n",
    "for name in tqdm(all_filenames):\n",
    "    with open(name+\".txt\") as f:\n",
    "        lines = f.readlines()\n",
    "        txt = \"\".join(lines)\n",
    "    tmp_dict = {}\n",
    "    for ix, i in enumerate(lines):\n",
    "        tmp_dict[ix] = {}\n",
    "        for ix2, j in enumerate(sent_tokenize(i.strip())):\n",
    "            tmp_dict[ix][ix2] = spacy_text_process(j.strip())\n",
    "    para_dict[name] = tmp_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d72ab70",
   "metadata": {},
   "source": [
    "# Using NeuralEDUSeg & Rules for EDU segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2956afe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Vacations are not only required by students but also by professionals and housekeepers .',\n",
       " 1: 'As it not only provide break to them from their regular commitment but also allows them to spend some time on themselves , heel stressful mind and boost desire of resuming to the regular schedule .',\n",
       " 2: 'Therefore schools should consider giving several small vocations to the students during the year .'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "para_dict[\"ArgumentAnnotatedEssays-2.0 3/brat-project-final/essay300\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "818a1420",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir(\"PE_paragraphs\")\n",
    "os.mkdir(\"PE_paragraphs_discourse_units\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "ccee4fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_path = \"/home/csgrad/sougatas/research_work/argumentation/PE_paragraphs/\"\n",
    "for fname, v in para_dict.items():\n",
    "    for para_id, sent_dict in v.items():\n",
    "        if len(sent_dict) > 0:\n",
    "            for sent_id, text in sent_dict.items():\n",
    "                if len(text.split()) >= 5:\n",
    "                    fn = tgt_path + fname.split(\"/\")[-1]+\"_\"+str(para_id)+\"_\"+str(sent_id)+\".txt\"\n",
    "                    with open(fn, 'w') as f:\n",
    "                        f.write(text)\n",
    "## Next Run the EDU segmentor on the files present in tgt_path, and save the output in the folder PE_paragraphs_discourse_units            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a561b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7101\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lr /home/csgrad/sougatas/research_work/argumentation/PE_paragraphs/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c67659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7101\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lr /home/csgrad/sougatas/research_work/argumentation/PE_paragraphs_discourse_units/ | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "458297ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence2list(sentence):\n",
    "    st = 0\n",
    "    punct = [\".\", \"?\", \"!\", \",\", \";\"]\n",
    "    lst = []\n",
    "    for ix, i in enumerate(sentence):\n",
    "        if i in punct:\n",
    "            if ix-1 >= 0 and ix+1 < len(sentence) and sentence[ix-1].isdigit() and sentence[ix+1].isdigit():\n",
    "                pass\n",
    "            else:\n",
    "                lst.append(sentence[st:ix + 1])\n",
    "                st = ix + 1\n",
    "\n",
    "    if st != len(sentence):\n",
    "        lst.append(sentence[st:])\n",
    "    \n",
    "    if len(lst) == 0:\n",
    "        lst.append(sentence)\n",
    "    return lst\n",
    "\n",
    "def punct_split(txt):\n",
    "    lst = sentence2list(txt)\n",
    "    min_thresh, pre = 3, \"\"\n",
    "    post_lst = []\n",
    "    for i in lst:\n",
    "        if len(i.split()) > min_thresh:\n",
    "            post_lst.append(pre+i)\n",
    "            pre = \"\"\n",
    "            \n",
    "        else:\n",
    "            if len(post_lst) > 0:\n",
    "                post_lst[-1] = post_lst[-1] + i\n",
    "                \n",
    "            else:\n",
    "                pre = pre+i\n",
    "\n",
    "    if pre != \"\":\n",
    "        post_lst.append(pre)\n",
    "\n",
    "    return post_lst\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58daf321",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_path = '/home/csgrad/sougatas/research_work/argumentation/PE_paragraphs_discourse_units/'\n",
    "processed_filenames = os.listdir(processed_path)\n",
    "len(processed_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cdac9bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 7100/7100 [00:00<00:00, 14776.63it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_text(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9 ]\", \" \", text).lower().strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "def fix_processing(txt):\n",
    "    txt = txt.replace(\"    \", \"   \").replace(\"I ' m\", \"I 'm\").replace(\"' ve \", \"'ve \")\n",
    "    txt = txt.replace(\"I ’ m\", \"I ’m\")\n",
    "    return txt\n",
    "\n",
    "def fix_short_lines(lst):\n",
    "    tmp_lines, pre = [], \"\"\n",
    "    for i in lst:\n",
    "        if len(process_text(i).split()) >= 2:\n",
    "            if process_text(pre) != \"etc\":\n",
    "                if pre != \"\":\n",
    "                    i = pre + \" \" +i\n",
    "                tmp_lines.append(i)\n",
    "            else:\n",
    "                tmp_lines[-1] = tmp_lines[-1]+ \" \"+pre\n",
    "                tmp_lines.append(i)\n",
    "            pre = \"\"\n",
    "        else:\n",
    "            if pre != \"\":\n",
    "                pre = pre + \" \" + i\n",
    "            else:\n",
    "                pre += i\n",
    "\n",
    "    if pre != \"\":\n",
    "        tmp_lines.append(pre)\n",
    "    return tmp_lines\n",
    "\n",
    "processed_para_dict = {}\n",
    "for name in tqdm(processed_filenames):\n",
    "    with open(processed_path+name) as f:\n",
    "        lines = f.readlines()\n",
    "        fname, para_id, sent_id = name.split(\"_\")\n",
    "        sent_id, _ = sent_id.split(\".\")\n",
    "        para_id, sent_id = int(para_id), int(sent_id)\n",
    "        lines = [fix_processing(i.strip()) for i in lines if len(i.strip()) > 0]\n",
    "        lines = [j.strip() for i in lines for j in split_conj_adv(i) if len(j.strip()) > 0]\n",
    "        lines = [j.strip() for i in lines for j in punct_split(i) if len(j.strip()) > 0]\n",
    "        lines = fix_short_lines(lines)\n",
    "        \n",
    "        if processed_para_dict.get(fname, None) is None:\n",
    "            processed_para_dict[fname] = {}\n",
    "        \n",
    "        if processed_para_dict[fname].get(para_id, None) is None:\n",
    "            processed_para_dict[fname][para_id] = {sent_id:lines}\n",
    "        else:\n",
    "            processed_para_dict[fname][para_id][sent_id] = lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ffe69f",
   "metadata": {},
   "source": [
    "# Collating Results & Formatting EDU with PE Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9ad505e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_boundary_dict(dct):\n",
    "    #Needs a dict para_id: {\"proc_text\":\"...\", \"proc_length\":xx, \"proc_start\":xx, \"proc_end\":xx}\n",
    "    boundary_dict = {}\n",
    "    for para_id, v in dct.items():\n",
    "        sents = sent_tokenize(v[\"text\"])#sent_tokenize(v[\"text\"])#sent_tokenize(v[\"proc_text\"])\n",
    "        tmp = {}\n",
    "        for ix, i in enumerate(sents):\n",
    "            i = spacy_text_process(i)\n",
    "            pre, post = v[\"proc_text\"].split(i, 1)\n",
    "            sent_st, sent_en = len(pre), v[\"proc_end\"] - v[\"proc_start\"] - len(post)\n",
    "\n",
    "            tmp[ix] = {\"sent\":i, \"sent_st\":sent_st, \"sent_en\":sent_en, \"offset\":v[\"proc_start\"]}\n",
    "        boundary_dict[para_id] = tmp\n",
    "        boundary_dict[para_id][\"raw_text\"] = v[\"proc_text\"]\n",
    "    return boundary_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e89c33d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edu_boundaries(sent, edu_list, st, en):\n",
    "    tmp = {}\n",
    "    for ix, edu in enumerate(edu_list):\n",
    "        pre, post = sent.split(edu, 1)\n",
    "        assert sent[len(pre): en - st - len(post)] == edu\n",
    "\n",
    "        tmp[ix] = {\"edu\": edu, \"st\":len(pre), \"en\":en - st - len(post)}\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87306739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'ArgumentAnnotatedEssays-2.0 3/brat-project-final/'\n",
    "all_filenames = [path+i.split(\".\")[0] for i in os.listdir(path) \\\n",
    "                 if (i.endswith(\"ann\") or i.endswith(\"txt\")) and i.startswith(\"essay\")]\n",
    "all_filenames = list(set(all_filenames))\n",
    "len(all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "d73dd7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from difflib import SequenceMatcher\n",
    "\n",
    "\n",
    "def get_pre_post(txt, span):\n",
    "    splt_lst = txt.split(span)\n",
    "\n",
    "    if len(splt_lst) == 2:\n",
    "        pre, post = splt_lst[0], splt_lst[1]\n",
    "    elif len(splt_lst) == 1:\n",
    "        pre, post = splt_lst[0], \"\"\n",
    "    elif len(splt_lst) > 2:\n",
    "        pre, post = \"NA\", \"NA\"\n",
    "        if splt_lst[0] == \"\":\n",
    "            pre = \"\"\n",
    "        if splt_lst[-1] == \"\":\n",
    "            post = \"\"\n",
    "    else:\n",
    "        pre, post = \"\", \"\"\n",
    "    return len(pre.strip()), len(post.strip())\n",
    "\n",
    "def check_span(str_a, str_b, span):\n",
    "    # a = reference, b = edu\n",
    "    if not any([i.startswith(\" \") or i.endswith(\" \") for i in str_a.split(span)]) and \\\n",
    "        not any([i.startswith(\" \") or i.endswith(\" \") for i in str_b.split(span)]) and \\\n",
    "        span != str_a and span != str_b:\n",
    "        return False\n",
    "    b_len_pre, b_len_post = get_pre_post(str_b, span)\n",
    "    a_len_pre, a_len_post = get_pre_post(str_a, span)\n",
    "\n",
    "    if (a_len_pre == 0 and a_len_post == 0) or (b_len_pre == 0 and b_len_post == 0):\n",
    "        return True\n",
    "    elif (a_len_post == 0 and b_len_pre == 0) or (a_len_pre == 0 and b_len_post == 0):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def match_sequence(str_a, str_b, add_lambda=False):\n",
    "    span, st, en, continuing = None, None, None, False\n",
    "\n",
    "    if not add_lambda:\n",
    "        s = SequenceMatcher(None, str_a, str_b)\n",
    "        if not any([block.a == 0 or block.b == 0 for block in s.get_matching_blocks()]):\n",
    "            s = SequenceMatcher(lambda x: x == \" \", str_a, str_b)\n",
    "            if not any([block.a == 0 or block.b == 0 for block in s.get_matching_blocks()]):\n",
    "                s = SequenceMatcher(None, str_a, str_b)\n",
    "    else:\n",
    "        s = SequenceMatcher(lambda x: x == \" \", str_a, str_b)\n",
    "    for block in s.get_matching_blocks():\n",
    "        if block.size > 0:\n",
    "            if block.a == 0:\n",
    "                span = str_a[block.a:(block.a + block.size)]\n",
    "                if check_span(str_a, str_b, span) or str_a.startswith(span) and str_a.endswith(span):\n",
    "                    st, en = block.b, block.b + block.size\n",
    "                    continuing = en == len(str_b)\n",
    "\n",
    "            elif block.b == 0:\n",
    "                span = str_b[block.b:(block.b + block.size)]\n",
    "                if check_span(str_a, str_b, span):\n",
    "                    st, en = block.b, block.b + block.size\n",
    "                    continuing = en == len(str_b)\n",
    "\n",
    "    if st is None or en is None:\n",
    "        span = None\n",
    "    return span, st, en, continuing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d30658cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 402/402 [01:05<00:00,  6.17it/s]\n"
     ]
    }
   ],
   "source": [
    "ann_dict, simple_ann_dict = {}, {}\n",
    "for name in tqdm(all_filenames):\n",
    "    with open(name+\".txt\") as f:\n",
    "        paragraphs = f.readlines()\n",
    "        txt = \"\".join(paragraphs)\n",
    "\n",
    "    \"\"\" Create Segments \"\"\"\n",
    "    segments = []\n",
    "    prev, prev_s= 0, 0\n",
    "    for ix, paragraph in enumerate(paragraphs):\n",
    "        spacy_proc_paragraph = \" \".join([spacy_text_process(i) for i in sent_tokenize(paragraph)])\n",
    "        orig_len, processed_len = len(paragraph), len(spacy_proc_paragraph)\n",
    "        segments.append([ix, paragraph, orig_len, prev, prev+len(paragraph), spacy_proc_paragraph, \n",
    "                         processed_len, prev_s, prev_s+len(spacy_proc_paragraph)])\n",
    "        prev += len(paragraph)\n",
    "        prev_s += len(spacy_proc_paragraph)\n",
    "    segments_df = pd.DataFrame(segments,columns=[\"p_id\", \"text\", \"orig_length\", \"orig_start\", \"orig_end\", \n",
    "                                                 \"proc_text\", \"proc_length\", \"proc_start\", \"proc_end\"])\n",
    "    segments_df = segments_df.set_index(\"p_id\")\n",
    "    segments_dict = segments_df.to_dict(orient=\"index\")\n",
    "    \n",
    "    sentence_boundary_dict = get_sentence_boundary_dict(segments_dict)\n",
    "    sentence_boundary_dict_cp = copy.deepcopy(sentence_boundary_dict)\n",
    "    \n",
    "    for para_id, v1 in sentence_boundary_dict.items():\n",
    "        for sent_id, v2 in v1.items():\n",
    "            if sent_id != \"raw_text\":\n",
    "                try:\n",
    "                    edu_list = processed_para_dict[name.split(\"/\")[-1]][para_id][sent_id]\n",
    "                except:\n",
    "                    edu_list = [v2[\"sent\"]]\n",
    "                edu_boundary = edu_boundaries(v2[\"sent\"], edu_list, v2[\"sent_st\"], v2[\"sent_en\"])\n",
    "                sentence_boundary_dict_cp[para_id][sent_id][\"edu_dict\"] = edu_boundary\n",
    "    ann_dict[name.split(\"/\")[-1]] = {\"segments\": sentence_boundary_dict_cp,\n",
    "                                    \"raw_text\":txt}\n",
    "    simple_ann_dict[name.split(\"/\")[-1]] = {}\n",
    "    for para_id, v2 in sentence_boundary_dict_cp.items():\n",
    "        tmp = []\n",
    "        for sent_id, v3 in v2.items():\n",
    "            if sent_id != \"raw_text\":\n",
    "                tmp.extend([v4[\"edu\"] for edu_id, v4 in v3[\"edu_dict\"].items()])\n",
    "        if len(tmp) > 0:  \n",
    "            simple_ann_dict[name.split(\"/\")[-1]][para_id] = {\"orig_text\": segments_dict[para_id][\"text\"], \n",
    "                                               \"orig_st\": segments_dict[para_id][\"orig_start\"],\n",
    "                                               \"orig_en\": segments_dict[para_id][\"orig_end\"],\n",
    "                                               \"edu_list\": tmp}\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3e5a18d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotations(dct):\n",
    "    tmp_dct = {}\n",
    "    for para_id, v1 in dct.items():\n",
    "        for i in list(zip(v1[\"edu_list\"], v1[\"edu_annotation\"])):\n",
    "            if tmp_dct.get(i[1][0], None) is None:\n",
    "                tmp_dct[i[1][0]] = i[0][i[1][1]:i[1][2]]\n",
    "            else:\n",
    "                tmp_dct[i[1][0]] = tmp_dct[i[1][0]] + \" \"+ i[0][i[1][1]:i[1][2]]\n",
    "    return {k:v for k,v in tmp_dct.items() if k != \"O\"}\n",
    "\n",
    "def match_annotations(name):\n",
    "    golden_dct = all_ann_dict[name]\n",
    "    golden_dct = {k:spacy_text_process(v[\"text\"]) for k,v in golden_dct.items()}\n",
    "    anno_dct = get_annotations(simple_ann_dict_bkp[name])\n",
    "    g_anns, a_anns = set(golden_dct.keys()), set(anno_dct.keys())\n",
    "    not_in_anno = g_anns - a_anns\n",
    "    not_in_gold = a_anns - g_anns\n",
    "    if len(not_in_anno) != len(not_in_gold) and (len(not_in_anno) != 0 or len(not_in_gold) != 0):\n",
    "        print(\"not_in_anno\", not_in_anno, \"not_in_gold\", not_in_gold)\n",
    "    assert len(not_in_anno) == len(not_in_gold) == 0\n",
    "    unmatched = []\n",
    "    for k,v in golden_dct.items():\n",
    "        if v != anno_dct[k]:\n",
    "            unmatched.append([name, k, v, anno_dct[k]])\n",
    "    return unmatched\n",
    "\n",
    "\n",
    "\n",
    "def merge_annotations(l1, l2, name):\n",
    "    #existing, new\n",
    "    lst = []\n",
    "    zipped_lst = list(zip(l1, l2))\n",
    "    for ix, i in enumerate(zipped_lst):\n",
    "        if i[0][0] == i[1][0] == \"O\":\n",
    "            lst.append(i[0])\n",
    "        elif i[0][0] != \"O\" and i[1][0] != \"O\" and i[0][0] != i[1][0]:\n",
    "            if (i[1][2] - i[1][1]) > (i[0][2] - i[0][1]):\n",
    "                lst.append(i[1])\n",
    "            elif (i[1][2] - i[1][1]) <= (i[0][2] - i[0][1]):\n",
    "                lst.append(i[0])\n",
    "            else:\n",
    "                lst.append(i[1])\n",
    "                print(\"\\nADDING Conflicting Annotations!!\",i,name,\"\\n\")\n",
    "        elif i[0][0] != \"O\":\n",
    "            lst.append(i[0])\n",
    "        else:\n",
    "            lst.append(i[1])\n",
    "    return lst     \n",
    "\n",
    "def default_annotation(lst):\n",
    "    return [[\"O\", 0, len(i)] for i in lst]\n",
    "\n",
    "def modify_annotations(lst):\n",
    "    modified_anno = [lst[0]]\n",
    "    for ix in range(1, len(lst)-1):\n",
    "        if lst[ix][0] != \"O\":\n",
    "            if modified_anno[ix-1][0] == lst[ix+1][0] and lst[ix][0] != lst[ix+1][0] and lst[ix+1][0] != \"O\":\n",
    "                modified_anno.append([lst[ix-1][0]] + lst[ix][1:])\n",
    "            else:\n",
    "                modified_anno.append(lst[ix])\n",
    "        else:\n",
    "            modified_anno.append(lst[ix])\n",
    "    if len(lst) > 1:\n",
    "        modified_anno.append(lst[-1])\n",
    "    assert len(modified_anno) == len(lst)\n",
    "    return modified_anno\n",
    "\n",
    "def modify_annotation_by_clusters(annotation):\n",
    "    t_df = pd.DataFrame(annotation, columns=[\"tag\", \"st\", \"en\"])\n",
    "    t_df[\"leng\"] = t_df[\"en\"]-t_df[\"st\"]\n",
    "    \n",
    "    # Assign sequence labels\n",
    "    st = t_df.iloc[0][\"tag\"]\n",
    "    seq = [1]\n",
    "    for ix, row in t_df[1:].iterrows():\n",
    "        if row[\"tag\"] != st:\n",
    "            seq.append(seq[-1] + 1)\n",
    "        else:\n",
    "            seq.append(seq[-1])\n",
    "        st = row[\"tag\"]\n",
    "    t_df[\"seq\"] = seq\n",
    "    \n",
    "    # Keep tags with largest total contiguous length\n",
    "    t_df_grp = t_df.groupby([\"tag\", \"seq\"]).agg({\"leng\":\"sum\"}).reset_index()\n",
    "    t_df_grp.columns = [\"tag\", \"seq\", \"cum_leng\"]\n",
    "    t_df = t_df.merge(t_df_grp, how=\"inner\", on=[\"tag\", \"seq\"])\n",
    "    \n",
    "    t_df_grp2 = t_df_grp.groupby([\"tag\"]).agg({\"cum_leng\":\"max\"}).reset_index()\n",
    "    t_df_grp2[\"keep\"] = 1\n",
    "    t_df_merged = t_df.merge(t_df_grp2, how=\"left\", on=[\"tag\", \"cum_leng\"]).fillna(0)\n",
    "    tag_lst = []\n",
    "    for ix, row in t_df_merged.iterrows():\n",
    "        if row[\"tag\"] != \"O\" and row[\"keep\"] == 0:\n",
    "            tag_lst.append([\"O\", 0, -1])\n",
    "        else:\n",
    "            tag_lst.append([row[\"tag\"], row[\"st\"], row[\"en\"]])\n",
    "    return tag_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d57e5c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_intersection(a, b):\n",
    "    return list(set(a.split()).intersection(set(b.split())))\n",
    "\n",
    "def get_phrases(lst):\n",
    "    phrase_list, conti = [], \"\"\n",
    "    for i in lst:\n",
    "        if i[1]:\n",
    "            if conti != \"\":\n",
    "                conti += \" \"+i[0]\n",
    "            else:\n",
    "                conti += i[0]\n",
    "        else:\n",
    "            if conti != \"\":\n",
    "                phrase_list.append(conti)\n",
    "                conti = \"\"\n",
    "    if conti != \"\":\n",
    "        phrase_list.append(conti)\n",
    "    return phrase_list    \n",
    "\n",
    "def get_matched_phrases(a, b):\n",
    "    matched_words = get_intersection(a, b)\n",
    "    a_matched = [[i, i in matched_words] for i in a.split()]\n",
    "    b_matched = [[i, i in matched_words] for i in b.split()]\n",
    "    a_matched_phrases, b_matched_phrases = list(set(get_phrases(a_matched))), list(set(get_phrases(b_matched)))\n",
    "    return a_matched_phrases, b_matched_phrases\n",
    "\n",
    "def span_type(st, en, leng):\n",
    "    if en-st == leng:\n",
    "        return \"full\"\n",
    "    elif st == 0:\n",
    "        return \"start\"\n",
    "    elif en == leng:\n",
    "        return \"end\"\n",
    "    else:\n",
    "        return \"mid\"\n",
    "    \n",
    "def get_edu_matching_span(phrase, edu):\n",
    "    phrase = phrase.replace(\"(\",\"\\(\").replace(\")\",\"\\)\").replace(\"+\",\"\\+\")\n",
    "    p = re.compile(phrase)\n",
    "    lst = []\n",
    "    for m in p.finditer(edu):\n",
    "        st, en = m.start(), m.end()\n",
    "        lst.append([m.group(), m.start(), m.end(), span_type(m.start(), m.end(), len(edu))])\n",
    "    return lst\n",
    "\n",
    "def check_valid_edu_match(ongoing, current):\n",
    "    if ongoing is None and current is not None:\n",
    "        return True\n",
    "    elif ongoing == \"end\" and current in [\"full\",\"start\"]:\n",
    "        return True\n",
    "    elif ongoing == \"full\" and current in [\"full\",\"start\"]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def get_unique_spans(lst_a, lst_b):\n",
    "    uniq_spans = []\n",
    "    for i in lst_a:\n",
    "        for j in lst_b:\n",
    "            if fuzz.token_set_ratio(i, j) == 100:\n",
    "                s = SequenceMatcher(None, i,j)\n",
    "                longest = s.find_longest_match(0, len(i), 0, len(j))\n",
    "                span = i[longest.a:longest.a+longest.size]\n",
    "                uniq_spans.append(span)\n",
    "    return list(set(uniq_spans))\n",
    "\n",
    "def correct_spelling(txt):\n",
    "    spelling_correction = {\"responsibl\":\"responsible\", \"communicatio\":\"communication\",\n",
    "                          \"educatio\":\"education\",\"environmen\":\"environment\", \"governmen\":\"government\"}\n",
    "    for k,v in spelling_correction.items():\n",
    "        if len(re.findall(\"\\\\b\"+k+\"\\\\b\", txt)) > 0:\n",
    "            txt = txt.replace(k, v)\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66cbabdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_edu_list_new(ann_txt, edu_lst, dev=False):\n",
    "    completed, lst, ongoing_span = [], [], None\n",
    "    for idx, edu in enumerate(edu_lst):\n",
    "        if \" \".join(completed) == ann_txt:\n",
    "            lst.append((None, None, None, False))\n",
    "            continue\n",
    "        tmp_ongoing_span = None\n",
    "        a_matched, b_matched = get_matched_phrases(ann_txt, edu)\n",
    "        uniq_spans = get_unique_spans(a_matched, b_matched)\n",
    "        if dev:\n",
    "            print(\"For EDU:\",edu,\".\\tUNIQ PHRASES:\",uniq_spans)\n",
    "        if len(uniq_spans) > 0 and len(completed) < len(ann_txt.split()):\n",
    "            tmp = []\n",
    "            flag = False\n",
    "            for phrase in uniq_spans:\n",
    "                edu_matching_spans = get_edu_matching_span(phrase, edu) #span, st, en, type: (mid, start, end, full)\n",
    "                for matching_span in edu_matching_spans:\n",
    "                    if dev:\n",
    "                        print(\"for phrase:\",phrase)\n",
    "                    if all([i[0] == i[1] for i in list(zip(ann_txt.split(), completed + phrase.split()))]) and \\\n",
    "                        len(phrase.split()) > len(tmp) and \\\n",
    "                        check_valid_edu_match(ongoing_span, matching_span[-1]): \n",
    "                    # The phrase aligns, is larger than existing phrase, and is valid\n",
    "                        tmp = phrase.split()\n",
    "                        tmp_ongoing_span = matching_span#matching_span[-1]\n",
    "                        flag = False\n",
    "                        if dev:\n",
    "                            print(\"inside IF\")\n",
    "                    elif all([i[0] == i[1] for i in list(zip(ann_txt.split(), phrase.split()))]) and \\\n",
    "                        len(phrase.split()) > len(tmp): \n",
    "                    # The phrase aligns, is larger than existing phrase, and is valid. But the completed align is bad\n",
    "                        tmp = phrase.split()\n",
    "                        tmp_ongoing_span = matching_span#matching_span[-1]\n",
    "                        flag = True\n",
    "                        if dev:\n",
    "                            print(\"inside ELIF\")\n",
    "                    else:\n",
    "                        if dev:\n",
    "                            print(\"inside ELSE\")\n",
    "                        pass\n",
    "            if len(tmp) > 0:\n",
    "                if flag:\n",
    "                    completed = [] # Reset completed\n",
    "                    lst[-1] = (None, None, None, False) #Reset last matched\n",
    "                completed.extend(tmp)\n",
    "                ongoing_span = tmp_ongoing_span[-1]\n",
    "                lst.append(tmp_ongoing_span[:-1]+[True if ongoing_span in [\"full\", \"end\"] else False])\n",
    "                if dev:\n",
    "                    print(\"TMP HAS VALUES:\",tmp,\".\\tONGOING SPAN:\",ongoing_span)\n",
    "                \n",
    "            else:\n",
    "                lst.append((None, None, None, False))\n",
    "                ongoing_span = None\n",
    "                completed = []\n",
    "                if dev:\n",
    "                    print(\"TMP IS EMPTY:\",tmp,\".\\tONGOING SPAN:\",ongoing_span)\n",
    "            \n",
    "        else:\n",
    "            lst.append((None, None, None, False))\n",
    "            ongoing_span = None\n",
    "            completed = []\n",
    "            if dev:\n",
    "                print(\"DIDNT ENTER IF-ELSE\")\n",
    "            \n",
    "        if ongoing_span is None or ongoing_span not in [\"full\", \"end\"]:\n",
    "            ongoing_span = None\n",
    "        if dev:\n",
    "            print(\"COMPLETED=>\",completed,\"\\n\")\n",
    "\n",
    "    return lst\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "85bd9764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_txt = spacy_text_process(all_ann_dict[\"essay184\"][\"T9\"][\"text\"])\n",
    "# edu_list = simple_ann_dict_bkp[\"essay184\"][3][\"edu_list\"]\n",
    "ann_txt = spacy_text_process(all_ann_dict[\"essay165\"][\"T11\"][\"text\"])\n",
    "edu_list = simple_ann_dict_bkp[\"essay165\"][4][\"edu_list\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 906,
   "id": "155ee1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I spent the time of biology class to do my Chemistry research and got A+ for both biology and chemistry',\n",
       " ['Secondly , students should not be required to attend classes',\n",
       "  'because they can manage the time',\n",
       "  'spending on classes',\n",
       "  'to do other benefit things',\n",
       "  'if they already understand topic in the class very well .',\n",
       "  'In my experience ,',\n",
       "  'many times I did not attend biology class',\n",
       "  'because my university does not require the students to attend classes .',\n",
       "  'Also , I had already studied all biology topics',\n",
       "  'before attending university .',\n",
       "  'As a result ,',\n",
       "  'I spent the time of biology class to do my Chemistry research',\n",
       "  'and got A+ for both biology and chemistry .'])"
      ]
     },
     "execution_count": 906,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_txt, edu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 912,
   "id": "75838cf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(None, None, None, False),\n",
       " (None, None, None, False),\n",
       " (None, None, None, False),\n",
       " (None, None, None, False),\n",
       " (None, None, None, False),\n",
       " (None, None, None, False),\n",
       " ['I', 11, 12, False],\n",
       " (None, None, None, False),\n",
       " ['I', 7, 8, False],\n",
       " (None, None, None, False),\n",
       " (None, None, None, False),\n",
       " ['I spent the time of biology class to do my Chemistry research',\n",
       "  0,\n",
       "  61,\n",
       "  True],\n",
       " ['and got A+ for both biology and chemistry', 0, 41, False]]"
      ]
     },
     "execution_count": 912,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_edu_list_new(ann_txt, edu_list, dev=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53923781",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 402/402 [00:00<00:00, 1263.29it/s]\n"
     ]
    }
   ],
   "source": [
    "all_ann_dict = {}\n",
    "\n",
    "for name in tqdm(all_filenames):\n",
    "    with open(name+\".ann\") as f:\n",
    "        ann = f.readlines()\n",
    "    tmp = {}\n",
    "    for i in ann:\n",
    "        ann_line = i.strip().split(\"\\t\")\n",
    "        \n",
    "        if ann_line[0].startswith(\"T\"):\n",
    "            typ, st, en = ann_line[1].split()\n",
    "            st, en = int(st), int(en)\n",
    "            tmp[ann_line[0]] = {\"type\": typ, \"start\":st, \"end\":en, \"text\":correct_spelling(ann_line[-1]),\n",
    "                               \"supports\":[], \"attacks\":[]}\n",
    "            \n",
    "        elif ann_line[0].startswith(\"A\"):\n",
    "            _, t_node, stance = ann_line[1].split()\n",
    "            tmp[t_node][\"stance\"] = stance\n",
    "            \n",
    "        elif ann_line[0].startswith(\"R\"):\n",
    "            typ, a1, a2 = ann_line[1].split()\n",
    "            a1, a2 = a1.split(\":\")[-1], a2.split(\":\")[-1]\n",
    "            tmp[a1][typ].append(a2)\n",
    "            \n",
    "        else:\n",
    "            print(\"\\nNOT RECOGNIZED!!\\n\")\n",
    "\n",
    "    all_ann_dict[name.split(\"/\")[-1]] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8374a67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_ann_dict_bkp = copy.deepcopy(simple_ann_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "05a804fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 402/402 [01:38<00:00,  4.06it/s]\n"
     ]
    }
   ],
   "source": [
    "all_unmatched = []\n",
    "for name in tqdm(all_filenames):\n",
    "    with open(name+\".ann\") as f:\n",
    "        ann = f.readlines()\n",
    "    \n",
    "    for i in ann:\n",
    "        ann_line = i.strip().split(\"\\t\")\n",
    "        \n",
    "        if ann_line[0].startswith(\"T\"):\n",
    "            typ, st, en = ann_line[1].split()\n",
    "            st, en = int(st), int(en)\n",
    "            ann_txt = spacy_text_process(ann_line[-1])\n",
    "            ann_txt = correct_spelling(ann_txt)\n",
    "            \n",
    "            for para_id, v1 in simple_ann_dict_bkp[name.split(\"/\")[-1]].items():\n",
    "                if v1.get(\"edu_annotation\", None) is None:\n",
    "                    simple_ann_dict_bkp[name.split(\"/\")[-1]][para_id][\"edu_annotation\"] = default_annotation(v1[\"edu_list\"])\n",
    "                    \n",
    "                if (v1[\"orig_en\"] > st and v1[\"orig_st\"] < en): \n",
    "                    edu_list = match_edu_list_new(ann_txt, v1[\"edu_list\"])\n",
    "                    annotation = []\n",
    "                    prev = False\n",
    "                    \n",
    "                    for ix2, j in enumerate(list(zip(edu_list, v1[\"edu_list\"]))):\n",
    "                        if ((prev or j[0][-1]) and j[0][0] is not None and len(j[0][0].strip().split()) > 1) or\\\n",
    "                            (j[0][0] is not None and j[0][0] in j[-1]):\n",
    "                            annotation.append([ann_line[0], j[0][1], j[0][2]])\n",
    "                            prev = j[0][-1]\n",
    "                        else:\n",
    "                            annotation.append([\"O\", 0, len(j[1])])\n",
    "\n",
    "                    simple_ann_dict_bkp[name.split(\"/\")[-1]][para_id][\"edu_annotation\"] = merge_annotations(simple_ann_dict_bkp[name.split(\"/\")[-1]][para_id][\"edu_annotation\"], \n",
    "                                                                                                        annotation, name)\n",
    "                    simple_ann_dict_bkp[name.split(\"/\")[-1]][para_id][\"edu_annotation\"] = modify_annotation_by_clusters(simple_ann_dict_bkp[name.split(\"/\")[-1]][para_id][\"edu_annotation\"])\n",
    "                \n",
    "\n",
    "    unm = match_annotations(name.split(\"/\")[-1])\n",
    "    if len(unm) > 0:\n",
    "        print(len(unm), \"Unmatched in\",name.split(\"/\")[-1],\"!\")\n",
    "    all_unmatched.extend(unm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "953b36a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'orig_text': 'Overall, it is easy to distinguish between a child having two languages and a child having just his native language while comparing their ability to communicate, understanding other studies, etc, to find that why children should start learning a foreign language in the beginning of the education.',\n",
       " 'orig_st': 1574,\n",
       " 'orig_en': 1871,\n",
       " 'edu_list': ['Overall , it is easy to distinguish between a child',\n",
       "  'having two languages and a child',\n",
       "  'having just his native language',\n",
       "  'while comparing their ability',\n",
       "  'to communicate ,',\n",
       "  'understanding other studies , etc ,',\n",
       "  'to find that why children should start learning a foreign language in the beginning of the education .'],\n",
       " 'edu_annotation': [['T4', 10, 51],\n",
       "  ['T4', 0, 32],\n",
       "  ['T4', 0, 31],\n",
       "  ['T4', 0, 29],\n",
       "  ['T4', 0, 16],\n",
       "  ['T4', 0, 33],\n",
       "  ['T3', 17, 100]]}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_ann_dict_bkp[\"essay327\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eae23178",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(simple_ann_dict_bkp, open(\"./PE_essays_formatted_edu_segments_v1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ed5343",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
